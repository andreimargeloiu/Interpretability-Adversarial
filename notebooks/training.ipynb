{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KJ-bpTO71J6J",
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training script\n",
    "\n",
    "It runs the training flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qi6cf5v71J7O"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2msNc36m1J7P",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# To enable importing robustness directory\n",
    "import sys, os\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "\n",
    "import torch as ch\n",
    "import numpy as np\n",
    "\n",
    "import cox.store\n",
    "from cox.utils import Parameters\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "from robustness import model_utils, datasets, train, defaults\n",
    "from robustness.datasets import CIFAR, HAM10000, HAM10000_dataset, HAM10000_3cls, HAM10000_dataset_3cls_balanced, freeze, unfreeze\n",
    "from robustness.tools.utils import fix_random_seed\n",
    "from robustness.evaluation import plot_curves_from_log, evaluate_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YcgX6KEU1J7K"
   },
   "source": [
    "## Training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DEPJ7sXzMaLq",
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "ADV_TRAIN = False\n",
    "ADV_EVAL = False\n",
    "lr = 1e-4\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "step_lr = None\n",
    "custom_schedule = None\n",
    "lr_patience = 5\n",
    "es_patience = 10\n",
    "\n",
    "# Model\n",
    "base_model_expid = None\n",
    "use_dropout_head = False\n",
    "dropout_perc = 0\n",
    "arch = 'resnet18'\n",
    "pytorch_pretrained = True\n",
    "unfreeze_to_layer = 0\n",
    "\n",
    "# Other settings\n",
    "do_eval_model = False\n",
    "eval_checkpoint_type = 'latest'\n",
    "TRAIN_COLAB = True\n",
    "NUM_WORKERS = 16\n",
    "expid = datetime.now().strftime(\"%Y-%m-%d---%H:%M:%S\")\n",
    "seed = 42\n",
    "\n",
    "# Ablation\n",
    "apply_ablation = False\n",
    "saliency_dir = None\n",
    "perc_ablation = 0\n",
    "\n",
    "# Adversary\n",
    "EPS = 0.5\n",
    "ITERATIONS = 7\n",
    "constraint = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1mpprSFxNkru"
   },
   "outputs": [],
   "source": [
    "if TRAIN_COLAB:\n",
    "    ds_path = #TODO-USER (e.g., \"/content/data\")\n",
    "    OUT_DIR = # TODO-USER (e.g., \"/content/drive/My Drive/logs\")\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    ds_path = # TODO-USER (e.g., \"/Users/andrei/Google Drive/data/HAM10000\")\n",
    "    OUT_DIR = # TODO-USER (e.g., \"/Users/andrei/Google Drive/logs\")\n",
    "    device = 'cpu'\n",
    "\n",
    "train_kwargs = {\n",
    "    'out_dir': \"train_out\",\n",
    "    'adv_train': ADV_TRAIN,\n",
    "    'adv_eval': ADV_EVAL,\n",
    "    'epochs': EPOCHS,\n",
    "    'lr': lr,\n",
    "    'optimizer': 'Adam',\n",
    "    'device': device,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'arch': arch,\n",
    "    'pytorch_pretrained': pytorch_pretrained,\n",
    "    'dataset_file_name': train_file_name,\n",
    "    'step_lr': step_lr,\n",
    "    'custom_schedule': custom_schedule,\n",
    "    'lr_patience': lr_patience,\n",
    "    'es_patience': es_patience,\n",
    "    'log_iters': 1,\n",
    "    'use_adv_prec': True,\n",
    "    'apply_ablation': apply_ablation,\n",
    "    'saliency_dir': saliency_dir,\n",
    "    'perc_ablation': perc_ablation,\n",
    "    'dropout_perc': dropout_perc,\n",
    "    'use_dropout_head': use_dropout_head\n",
    "}\n",
    "\n",
    "attack_kwargs = {\n",
    "    'constraint': constraint,\n",
    "    'eps': EPS,\n",
    "    'attack_lr': EPS/5,\n",
    "    'attack_steps': ITERATIONS,\n",
    "    'random_start': True\n",
    "}\n",
    "\n",
    "# merge train_kwargs with attack_kwargs\n",
    "train_kwargs_merged = {**train_kwargs, **attack_kwargs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sQegws_wvpHf"
   },
   "outputs": [],
   "source": [
    "fix_random_seed(seed)\n",
    "out_store = cox.store.Store(OUT_DIR, expid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xoGIxjmWNj3R"
   },
   "outputs": [],
   "source": [
    "print(out_store.exp_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zum9vzzE2Cmt"
   },
   "source": [
    "### Resume path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v94kOw-a2B86"
   },
   "outputs": [],
   "source": [
    "train_kwargs_merged['base_model_expid'] = base_model_expid\n",
    "if base_model_expid:\n",
    "  resume_path = os.path.join(OUT_DIR, base_model_expid, \"checkpoint.pt.latest\")\n",
    "else:\n",
    "  resume_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XSBv8YXRvpHi",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zfB9FmEXvpHj"
   },
   "source": [
    "Fill whatever parameters are missing from the defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g_G976xhvpHj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_args = Parameters(train_kwargs_merged)\n",
    "train_args = defaults.check_and_fill_args(train_args,\n",
    "                        defaults.TRAINING_ARGS, HAM10000)\n",
    "train_args = defaults.check_and_fill_args(train_args,\n",
    "                        defaults.PGD_ARGS, CIFAR)\n",
    "\n",
    "train_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Re2Qbj4tvpHr"
   },
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V8wnBqZevBDM"
   },
   "outputs": [],
   "source": [
    "dataset = HAM10000_3cls(ds_path, file_name=train_file_name, \n",
    "                        apply_ablation=apply_ablation, saliency_dir=saliency_dir, perc_ablation=perc_ablation,\n",
    "                        use_dropout_head=use_dropout_head, dropout_perc=dropout_perc)\n",
    "\n",
    "train_loader, val_loader = dataset.make_loaders(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    workers=NUM_WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JMXUjWzWIJ-W"
   },
   "outputs": [],
   "source": [
    "model, _ = model_utils.make_and_restore_model(\n",
    "    arch=arch,\n",
    "    pytorch_pretrained=pytorch_pretrained,\n",
    "    dataset=dataset, \n",
    "    resume_path=resume_path,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if base_model_expid == None: # if no base model, then train only the last layers\n",
    "    freeze(model.model)\n",
    "    unfreeze(model.model.fc, 5)\n",
    "else: # if base model, then unfreeze until a given layer to fine-tune the whole network\n",
    "    model = model.module\n",
    "    unfreeze(model.model, unfreeze_to_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PGkrVM4fcZ2V"
   },
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W1Jw2bXovpH1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "model_finetuned = train.train_model(train_args, model, (train_loader, val_loader), store=out_store)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Training took %.2f sec\" % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w-zmaOOlcCyK"
   },
   "outputs": [],
   "source": [
    "plot_curves_from_log(out_store)['logs'].df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qkM4cqXNXy5w"
   },
   "outputs": [],
   "source": [
    "print(out_store.exp_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UGNWwXm3lvdU"
   },
   "outputs": [],
   "source": [
    "out_store.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model\n",
    "\n",
    "Evaluate the model on the whole train set and the test set (on standard data, with ablation as in training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_eval_model:\n",
    "    # training dataset\n",
    "    train_dataset = HAM10000_dataset_3cls_balanced(ds_path, train_file_name, train=True, \n",
    "                                                   transform = dataset.transform_test, \n",
    "                                                   apply_ablation=apply_ablation, saliency_dir=saliency_dir, \n",
    "                                                   perc_ablation=perc_ablation)\n",
    "\n",
    "    # test dataset\n",
    "    test_dataset = HAM10000_dataset_3cls_balanced(ds_path, test_file_name, test=True,\n",
    "                                                  transform = dataset.transform_test,\n",
    "                                                  apply_ablation=apply_ablation, saliency_dir=saliency_dir, \n",
    "                                                  perc_ablation=perc_ablation)\n",
    "\n",
    "    accs = evaluate_model(out_store.exp_id, dataset, train_dataset, test_dataset, OUT_DIR, device, arch, checkpoint_type=eval_checkpoint_type)\n",
    "    print(accs)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Tags",
  "colab": {
   "name": "train_ham10000.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}